Installation steps (excluding most of AWS-specific procedures including security, S3, lambda, IAM, EFS, SQS, SES) 

For EFS, setup security group with port 2049 for access from both front end server and back end server. 

Setup IAM user with appropriate access to S3, for AWS CLI tool. Same for lambda, etc 

----Setting up front end server server 

Launch EC2
 
	Ubuntu Server 18.04 LTS (HVM), SSD Volume Type

	Instance: t2.xlarge
	
	Set IAM role from the above step
	
	Set to unlimited CPU credit use mode
	  
	Open inbound port 22 (SSH), port 80 (HTTP), port 443 (HTTPS),Custom TCP	TCP 5672 (RabbitMQ) 
	
	Set EBS volume to 20GB 
	
	Set the elastic IP address  
 
On the ssh window, run the following commands.

sudo apt-get update

sudo apt-get install apache2 

sudo apt-get install libapache2-mod-wsgi-py3

sudo apt-get install python3-setuptools

sudo apt install unzip

sudo apt-get install -y imagemagick-6.q16

sudo vim ~/.bashrc

------------ Using vim, put in the following three lines---------------

alias re="sudo /etc/init.d/apache2 restart"

alias ee="sudo tail -20 /var/log/apache2/error.log"

alias dc="sudo tail -20 /etc/apache2/sites-available/django.conf"

-----------------------------------------------------------------------

bash

sudo mkdir /var/src 

wget https://media.djangoproject.com/releases/2.2/Django-2.2.12.tar.gz 

sudo tar -xf Django-2.2.12.tar.gz

cd Django-2.2.12/

sudo python3 setup.py install

cd       


SFTP transfer or git transfer the macroMS front end source codes

Unzip them 
 
cd /etc/apache2/sites-available/

sudo vim django.conf

-----------------Using vim put in the following----------------------
  
<VirtualHost *:80>

Alias /static/ /home/ubuntu/website/static/

<Directory /home/ubuntu/website/static/>

Require all granted

</Directory>

WSGIScriptAlias / /home/ubuntu/website/website/wsgi.py

ServerName  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX           <--------Replace with address like: ec2-12-297-34-589.us-west-2.compute.amazonaws.com 

<Directory "/home/ubuntu/website/website">

<Files wsgi.py>
 
Require all granted

</Files>

</Directory>

</VirtualHost>


------------------exit vim. Run commands below ----------------

sudo a2ensite django.conf

re

sudo apt-get update

sudo apt-get install software-properties-common

sudo add-apt-repository ppa:certbot/certbot

sudo apt-get update

sudo apt-get install python-certbot-apache

sudo certbot --apache -d macroms.scs.illinois.edu

sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev   

sudo bash
 
wget https://www.python.org/ftp/python/3.5.6/Python-3.5.6.tgz

tar -xf Python-3.5.6.tgz

cd Python-3.5.6/    

./configure

make

make install 

****************Now, check python3 invokes python 3.5.6

cd 

pip3 install --upgrade pip

pip3 install celery==4.4.2

sudo apt-get install rabbitmq-server

sudo rabbitmqctl add_user worker1 XXXXXXX     <------ Replace XXXXX with password 

sudo rabbitmqctl add_vhost macroMS_host

sudo rabbitmqctl set_permissions -p macroMS_host worker1 ".*" ".*" ".*"

sudo service rabbitmq-server stop

sudo service rabbitmq-server start

vim website/go/celery_worker.py

##############change YYYYYYYYYY to private DNS from AWS EC2 console for this instance. Example value looks like 10.121.151.111. XXXXXXX is the password setup at the above step. 

from celery import Celery

import os,sys

from celery.utils.log import get_task_logger

logger = get_task_logger(__name__)

app = Celery('tasks', backend='amqp',broker='amqp://worker1:XXXXXXXXXX@YYYYYYYYYY/macroMS_host')


@app.task

def run_cmd(cmd):

     output=os.popen(cmd).read()

     print(cmd)

     print(output)

     return True
     
###########################################



git clone https://github.com/aws/efs-utils

sudo apt-get -y install binutils

cd efs-utils/

./build-deb.sh

sudo apt-get -y install ./build/amazon-efs-utils*deb

cd

mkdir efs

sudo chmod XXX efs           <=== Set the appropriate number for XXX. Example value looks like 233

sudo mount -t efs -o tls XXXXXXXXXXXXX:/ efs        <=== Set the appropriate address for the Elastic File System storage for XXXXXXXXXXXXX. Example value looks like fs-3gc2gh22

sudo nohup python /home/ubuntu/website/go/RUN_THIS_CODE_ALWAYS_AND_IN_SUDO.py & 



####################################################################### set up the backened worker notes#########################################################

Launch EC2 

	Ubuntu Server 18.04 LTS (HVM), SSD Volume Type
	
	Instance: t2.medium
	
	Set IAM role 
	
	Set to unlimited CPU credit use mode
	
        Open port 22,5672,443	
	
	Set EBS volume to 8GB

        In IAM, setup access privileges for Elastic File System
  	 

 
sudo apt-get update

sudo apt-get install unzip

sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev   

sudo bash

wget https://www.python.org/ftp/python/3.5.6/Python-3.5.6.tgz

tar -xf Python-3.5.6.tgz

cd Python-3.5.6/    

./configure

make

make install 

****************Now, check python3 invokes python 3.5.6

cd 

pip3 install --upgrade pip

pip3 install numpy==1.14.0

pip3 install matplotlib==2.1.1

pip3 install openslide_python==1.1.1

pip3 install xlsxwriter==1.0.2

pip3 install scipy==1.0.0

pip3 install scikit-image==0.13.1

pip3 install pyserial==3.4 

pip3 install PyQt5==5.9.2

pip3 install openslide-python==1.1.1

pip3 install opencv-python==4.2.0.32

pip3 install requests==2.23.0

pip3 install opencv-python==4.2.0.32

sudo apt-get install python-openslide 

sudo apt-get install gtk-doc-tools gobject-introspection build-essential pkg-config glib2.0-dev libexpat1-dev

sudo apt-get install libffi-dev libtiff5-dev libjpeg-turbo8-dev libgsf-1-dev

wget https://github.com/libvips/libvips/releases/download/v8.9.1/vips-8.9.1.tar.gz

tar -xf vips-8.9.1.tar.gz

cd vips-8.9.1

./configure

make

sudo make install

sudo ldconfig

cd

sudo apt-get install -y imagemagick-6.q16

rm -rf * 

mkdir data

Now, put the macroMS backend source code into the home directory  

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

unzip awscliv2.zip

sudo ./aws/install

pip3 install celery==4.4.2

cd 

vim celery_worker.py

 
####### Use the same XXXXXX and YYYYYYYYYYYYYYY used as above ############

from celery import Celery

import os,sys

from celery.utils.log import get_task_logger

logger = get_task_logger(__name__)

app = Celery('tasks', backend='amqp',broker='amqp://worker1:XXXXXX@YYYYYYYYYYYYYYY/macroMS_host')


@app.task

def run_cmd(cmd):

    output=os.popen(cmd).read()

    print(cmd)

    print(output)

    return True

##################################


Now launch two celery workers  
 
mkdir data1

cp celery_worker.py data1

cd data1

sudo nohup celery -A celery_worker worker -Q feature_recognition,celery -n wk1 -f /home/ubuntu/celery_featurereq1.txt &


mkdir data2

cp celery_worker.py data2

cd data2

sudo nohup celery -A celery_worker worker -Q feature_recognition,celery -n wk2 -f /home/ubuntu/celery_featurereq2.txt &
 
git clone https://github.com/aws/efs-utils

sudo apt-get -y install binutils

 cd efs-utils/

./build-deb.sh

sudo apt-get -y install ./build/amazon-efs-utils*deb

cd

mkdir efs

sudo chmod XXX efs        Use appropriate XXX value 

sudo mount -t efs -o tls XXXXXXXXXXXXXXXXXX:/ efs    For XXXXXXXXXXXXXXXXXX, use the same EFS storage ID used as above.

Above steps create one worker node. To add more workers, exact same steps can be performed. Alternatively, AMI can be created + launched. The celery workers may need to be relaunched when an AMI gets created from the current worker node. 
 
 

########################### Setup data analysis node, for creating AMI Image for launching nodes ####################################

Launch a node

	Ubuntu Server 18.04 LTS (HVM), SSD Volume Type. 
 
	T2.small (For installation purpose, needs more RAM and resources. Nano will be launched later).

 	Storage size should be 25GB
	
	Open port 22 

sudo apt-get update 

sudo apt install python

sudo add-apt-repository universe

sudo apt update 

curl https://bootstrap.pypa.io/get-pip.py --output get-pip.py

sudo python2 get-pip.py

pip install --user scipy==1.2.3

pip install --user numpy==1.16.6
 

****** make image and launch nano instance for the image. Storage size should be 25GB, with port 22 opened

sudo apt-get install unzip

pip install --user XlsxWriter==1.3.3

pip install --user pymzML==0.7.9
 
pip install boto3

sudo apt install wine-stable

sudo dpkg --add-architecture i386

sudo apt-get update

sudo apt-get install wine32
 
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

unzip awscliv2.zip

sudo ./aws/install

aws configure

######################

In the IAM->User->Your user->security credentials->create access keys-> copy and paste credentials 

Set default region, which is the region where EC2 instances are located at: i.e. us-east-1

##################

cd

vim .aws/config 

ADD BELOW (Limit RAM usaged for the nano node) 
##################

s3 =
  max_concurrent_requests = 1

##################
 

sudo apt-get install gtk-doc-tools gobject-introspection build-essential pkg-config glib2.0-dev libexpat1-dev

sudo apt-get install libffi-dev libtiff5-dev libjpeg-turbo8-dev libgsf-1-dev

wget https://github.com/libvips/libvips/releases/download/v8.9.1/vips-8.9.1.tar.gz

tar -xf vips-8.9.1.tar.gz

cd vips-8.9.1

./configure

make

sudo make install

sudo ldconfig

sudo apt-get install -y imagemagick-6.q16

cd 

sudo rm -rf *

Then, upload and unzip the backup file for the data analysis node

Create the image 

Update lambda function "S3_to_EC2_macroMS" with the new AMI ID, ie) ami-234j840sk51lsu

---

ec2.create_instances(
     ImageId='XXXXXXXXXXXXXXXX', ...

---
 
 